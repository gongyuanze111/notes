越多的参数挪到CPU上，GPU的负担就越小；但随之的代价就是，更为频繁的CPU，GPU交互，极大增加了训练推理的时间开销。因此，DeepSpeed使用的一个核心要义是，时间开销和显存占用的权衡。

Optimizer state partitioning (ZeRO stage 1)  只对optimizer进行切片后分布式保存
Gradient partitioning (ZeRO stage 2)   对optimizer和grad进行切片后分布式保存
Parameter partitioning (ZeRO stage 3)  对optimizer、grad和模型参数进行切片后分布式保存

对优化器的理解
优化器（例如Adam）保存了模型参数的梯度（一阶动量）和二阶动量，因此，优化器保存的梯度会占用大量显存，因此，ZeRO stage 1和ZeRO stage 2的优化器切片，可以减少优化器的显存占用。
